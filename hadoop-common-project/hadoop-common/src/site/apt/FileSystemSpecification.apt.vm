~~ Licensed under the Apache License, Version 2.0 (the "License");
~~ you may not use this file except in compliance with the License.
~~ You may obtain a copy of the License at
~~
~~   http://www.apache.org/licenses/LICENSE-2.0
~~
~~ Unless required by applicable law or agreed to in writing, software
~~ distributed under the License is distributed on an "AS IS" BASIS,
~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
~~ See the License for the specific language governing permissions and
~~ limitations under the License. See accompanying LICENSE file.

  ---
  Hadoop FileSystem Specification
  ---
  ---
  ${maven.build.timestamp}

Apache Hadoop FileSystem Specification

~~ %{toc|section=1|fromDepth=0}

~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* {Introduction}

  This document defines the expected behaviors of a Hadoop-compatible filesystem,
  including many that aren't (or can't be) explicitly tested -yet.
  
  Most of the Hadoop operations are tested against HDFS in the Hadoop test
  suites, usually throogh <<<MiniDFSCluster>>>. Before a version of Apache Hadoop
  is lreased
   is used so ubiquitously. HDFS's actions have
  been modeled closely on the POSIX Filesystem behavior -using the actions and
  return codes of Unix filesystem actions as a reference.
  
  What is not so rigorously tested is how well other filesystems accessible from
  Hadoop behave. the bundled S3 filesystem makes Amazon's S3 blobstore accessible
  through the FileSystem API. The Swift filesystem driver provides similar
  functionality for the OpenStack Swift blobstore. The Azure object storage
  filesystem in branch-1-win talks to Microsoft's Azure equivalent. All of these
  bind to blobstores, which do have different behaviours, especially regarding
  consistency guarantees, and atomicity of operations.
  
  The Local filesystem provides access to the underlying filesystem of the
  platform -its behaviour is defined by the operating system -and again, can
  behave differently from HDFS.
  
  Finally, there are filesystems implemented by third parties, that assert
  compatibility with Apache Hadoop. There is no formal compatibility suite, and
  hence no way for anyone to declare compatibility except in the form of their
  own compatibility tests.
  
  This document does not attempt to formally define compatibility; passing the
  associated test suites does not guarantee correct behavior in MapReduce jobs,
  or HBase operations.
  
  What the test suites do define is the expected set of actions -failing these
  tests will highlight potential issues.
  
** Naming

  This document follows RFC2119 rules regarding the use of MUST, MUST NOT, MAY,
  and SHALL -and MUST NOT be treated as normative.
  
  
* Assumptions contained by users of the Hadoop FileSystem APIs
 
  The original <FileSystem> class and its usages are based on a set of
  assumptions "so obvious that nobody wrote them down" -primarily that HDFS is
  the underlying filesystem, and that it offers a subset of the behavior of a
  Posix filesystem
    
    * It's a hierarchical directory structure with files and directories.
    
    * Files contain data -possibly 0 bytes worth.
    
    * You cannot put files or directories under a file
    
    * Directories contain 0 or more files
    
    * A directory entry has no data itself
    
    * You can write arbitrary binary data to a file -and when that file's contents
     are read in, from anywhere in or out the cluster -that data is returned.
    
    * You can store many GB of data in a single file.
    
    * The root directory, <<<"/">>>, always exists, and cannot be renamed. It is always a
      directory, and cannot be overwritten by a file write operation. An attempt to
      recursively delete the root directory will delete its contents (assuming
      permissions allow this), but will retain the root path itself.
    
    * You cannot rename/move a directory under itself.
    
    * You cannot rename/move a directory atop any existing file other than the
      source file itself.
    
    * Security: If you don't have the permissions for an operation, it will fail
      with some kind of error.

** Path Names
  
    * A path is comprised of path components separated by <<<'/'>>>.
    
    * Paths are compared based on unicode code-points. 
    
    * Case-insensitive and locale-specific comparisons MUST NOT not be used.
    
    * A path element is a unicode string of 1 or more characters.
    
    * Path element MUST NOT include the characters <<<":">>> or <<<"/">>>.
    
    * Path element SHOULD NOT include characters of ASCII/UTF-8 value 0-31 .
    
    * Path element MUST NOT be <<<".">>>  or <<<"..">>> 
    
    * Note also that the Azure blob store documents say that paths SHOULD NOT use
     a trailing <<<".">>> (as their .NET URI class strips it).

** Security Assumptions

  Except in the special section on security, this document assumes the client has
  full access to the filesystem. Accordingly, the majority of items in the list
  do not add the qualification "assuming the user has the rights to perform the
  operation with the supplied parameters and paths"
  
  The failure modes when a user lacks security permissions are not specified.

** Networking Assumptions
  
  This document assumes this all network operations succeed -all statements
  can be assumed to be qualified as <"assuming the operation does not fail due
  to a network availability problem">
  
  * The final state of a filesystem after a network failure is undefined.
  
  * The immediate consistency state of a filesystem after a network failure is undefined.
  
  * If a network failure can be reported to the client, the failure MUST be an
    instance of <<<IOException>>>
  
  * The exception details SHOULD include diagnostics suitable for an experienced
    Java developer _or_ operations team to begin diagnostics. For example: source
    and destination hostnames and ports on a ConnectionRefused exception.
  
  * The exception details MAY include diagnostics suitable for inexperienced
    developers to begin diagnostics. For example Hadoop tries to include a
    reference to {{{http://wiki.apache.org/hadoop/ConnectionRefused}}} when a TCP
    connection request is refused.

~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Core requirements of a Hadoop Compatible Filesystem

  Here is the what a filesystem is expected to do. Some filesystems do not
  meet all these requirements. As a result, some programs may not work as
  expected. 

** Atomicity

  * Rename of a file MUST be atomic.
  
  * Rename of a directory SHOULD be atomic. Blobstore filesystems MAY offer
  non-atomic directory renaming.
  
  * Delete of a file MUST be atomic.
  
  * Delete of an empty directory MUST be atomic.
  
  * Recursive directory deletion MAY be atomic. Although HDFS offers atomic
  recursive directory deletion, none of the other FileSystems that Hadoop supports
  offers such a guarantee -including the local filesystems.
  
  * <<<mkdir()>>> SHOULD be atomic.
  
  * <<<mkdirs()>>> MAY be atomic. [It is *currently* atomic on HDFS, but this is not
  the case for most other filesystems -and cannot be guaranteed for future
  versions of HDFS]
  
  * If <<<append()>>> is implemented, each individual <<<append()>>> 
  operation SHOULD be atomic.
  
  * <<<FileSystem.listStatus()>>> does contain any guarantees of atomicity,
    though some uses in the MapReduce codebase (such as <<<FileOutputCommitter>>>) do
    assume that the listed directories do not get deleted between listing their
    status and recursive actions on the listed entries.

** Consistency

  The consistency model of a Hadoop filesystem is <one-copy-update-semantics>;
  that generally that of a traditional Posix filesystem. (Note that NFS relaxes
  some constraints about how fast changes propagate)
  
  * Create: once the <<<close()>>> operation on an output stream writing a newly
  created file has completed, in-cluster operations querying the file metadata
  and contents MUST immediately see the file and its data.
  
  *  Update: Once  the <<<close()>>>  operation on  an output  stream writing  a newly
  created file  has completed,  in-cluster operations  querying the  file metadata
  and contents MUST immediately see the new data.
  
  *  Delete:   once  a  <<<delete()>>>   operation  is   on  a  file   has  completed,
  it MUST NOT be visible or accessible. Specifically
  <<<listStatus()>>>, <<<open()>>>,<<<rename()>>> and <<<append()>>>
   operations MUST fail.
  
  * When file is deleted then a new file of the same name created, the new file
   MUST be immediately visible.
   
  * Rename: after a <<<rename()>>>  has completed, operations against the new  path MUST
  succeed; operations against the old path MUST fail.
  
  * The consistency semantics out of cluster  MUST be the same as that in-cluster:
  All clients querying a file that is not being actively manipulated MUST see the
  same metadata and data irrespective of their location in or out of the cluster.

** Concurrency

  * The data added to a file during a write or append MAY be visible while the
    write operation is in progress.
  
  * If a client opens a file for a <<<read()>>> operation while another <<<read()>>>
    operation is in progress, the second operation MUST succeed. Both clients
    MUST have a consistent view of the same data.
  
  * If a file is deleted while a <<<read()>>> operation is in progress, the
    <<<delete()>>> operation SHOULD complete successfully. Implementations MAY cause
    <<<delete()>>> to fail with an IOException instead.
   
  * If a file is deleted while a <<<read()>>> operation is in progress, the <<<read()>>>
    operation MAY complete successfully. Implementations MAY cause <<<read()>>>
    operations to fail with an IOException instead.
  
  * Multiple writers MAY open a file for writing. If this occurs, the outcome
  is undefined
  
  * Undefined: action of <<<delete()>>> while a write or append operation is in
    progress

** Undefined limits

  Here are some limits to filesystem capacity that have never been explicitly
  defined.
  
    [[1]] The maximum # of files in a directory.
    
    [[1]] Max # of directories in a directory
    
    [[1]] Maximum total number of entries (files and directories) in a filesystem.
    
    [[1]] Max length of a filename (HDFS: 8000)
    
    [[1]] <<<MAX_PATH>>> - the total length of the entire directory tree referencing a
    file. Blobstores tend to stop at ~1024 characters
    
    [[1]] max depth of a path (HDFS: 1000 directories)
    
    [[1]] The maximum size of a single file

** Undefined timeouts

  Timeouts for operations are not defined at all, including:
  
    * The maximum completion time of blocking FS operations.
    MAPREDUCE-972 shows how <<<distcp>>> broke on slow s3 renames.
    
    * The timeout for idle read streams before they are closed.
    
    * The timeout for idle write streams before they are closed.
  
  The blocking-operation timeout is in fact variable in HDFS, as sites and
  clients may tune the retry parameters so as to convert filesystem failures and
  failovers into pauses in operation. Instead there is a general assumption that
  FS operations are "fast but not as fast as local FS operations", and that data
  reads and writes take a time that scales with the volume of data. This
  assumption shows a more fundamental one: the filesystem is "close" to the
  client applications as far as network latency and bandwidth is concerned.
  
  There are also some implicit assumptions about the overhead of some operations

    [[1]] <<<seek()>>> operations are fast and incur little or no network delays. [This 
    does not hold on blob stores]
    
    [[1]] Directory list operations are fast for directories with few entries.
    
    [[1]] Directory list operations are fast for directories with few entries, but may
    incur a cost that is O(no. of entries). Hadoop 2 added iterative listing to
    handle the challenge of listing directories with millions of entries without
    buffering.
    
    [[1]] A <<<close()>>> of an <<<OutputStream>>> is fast, irrespective of whether or not
    the file operation has succeeded or not.
    
    [[1]] The time to delete a directory is independent of the size of the number of 
    child entries

  Different filesystems not only have different behavior, under excess load or failure
  conditions a filesystem may behave very differently. 

~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Core specification of a Hadoop filesystem

  This section attempts to model the contents a filesystem as a set of paths that 
  are either directories, symbolic links or files; only files may contain data.
  
  The operations that a filesystem supports either examines the paths and
  reference data, or updates it.

** Definitions

  <iff> is shorthand for <if and only if>

  <<<length(F: Path): Int>>> is a function which returns the length of a File, F

  <<<exists(F: Path): boolean>>> is a function which returns <<<true>>> iff
  the path resolves to an file, directory or other entity within the filesystem.

  <<<isDir(F: Path): boolean>>> is a function which returns <<<true>>> iff
  <<<exists(F)>>> and F represents a directory.

  <<<isFile(F: Path): boolean>>> is a function which returns <<<true>>> iff
  <<<exists(F)>>> and F represents a file.

  <<<isSymLink(F: Path): boolean>>> is a function which returns <<<true>>> iff
  <<<exists(F)>>> and F represents a symbolic link.
  
  <<<isRoot(F: Path): boolean>>> is a function which returns <<<true>>> iff
  F is the root directory of a filesystem.
  
  <<<isAncestor(P1: Path, P2: Path): boolean>>> is a function which returns
   <<<true>>> iff P1 is a parent or ancestor of P2 in the file hierarchy
   -irrespective of whether or not either path exists

  <<<pos(S: Stream): int>>> is a function which returns the position of an
  input stream reading a file.

  <<<min(X: T, Y:T):T >>> is a function which returns the smallest of <(X,Y)> 
  for a type in the set <<<(integer, long)>>>
  
  <<<parent(P: Path): Path)>>> is a function that returns the parent of a path
  <<<data(P: Path): byte[])>>> is a function that, for any path
  where <<<exists(P)>>>, returns the data referenced by that path. If there
  is no data, an empty array <<<[]>>> is returned.
 
** A Model of Paths, Directories and Files
 
  A <Filesystem>, <FS> contains a finite set of <Path> elements that
  refer to filesystem entities that MUST include
  Files and Directories and MAY include symbolic links. Filesystems MAY
  reference other entities (devices, named pipes, etc.)
  

  A Path can be represented as a list of <Path_Elements>. The set
  of all possible paths is <Paths>
  
-----------

  forall P in Paths :- P==[]
                    || P==[ H | T] where H in Path_Elements && T in Paths.
-----------
  
  The path represented by empty list, <<<[]>>> is the <root path>, and is 
  notated by the string "/"
  
    The function <parent(Path):Path> can be defined recursively
  
-----------

  parent([])      ::== nil
  parent([T])     ::== []
  parent([H | T]) ::== [H | parent(T)]
  
-----------
  
  Path elements are non-empty strings. The exact set of valid strings MAY 
  be specific to a particular filesystem implementation.
  
  Path Elements MUST NOT be in ["", ".",  "..", "/"]

  Path Elements MUST NOT contain the strings "/" and ":".
  
  Filesystems MAY have other strings that are not permitted in a path element.
  
  The last Path Element in a Path is called the filename:
  
-----------

  filename([]) = undefined
  filename([H]) = H
  filename([H|T]) = filename(T)

-----------

  The childElements of a path D that is a descendent of path P 
  is the list of path elements in D that follow the path P
  
  childElements([],C) = C
  childElements(P,[]) => error
  childElements([H|T],[H|T2]) = childelements(T, T2)
  # the outcome is not defined if the path D is not equal to or
  # a descendent of path P
  childElements([H2|T],[H|T2]) where H2 != H ==>  error
  

  
  A path MAY refer to a directory
  
-----------

  exists P in Paths where isDir(P)

-----------
  
  A path MAY refer to a file
    
-----------

  exists P in Paths where isFile(P)

-----------

  A path MAY refer to a symbolic link
    
-----------

  exists P in Paths where isSymlink(P)

-----------


  A filesystem contains a finite subset of all possible Paths
  
-----------

  paths(FS) ::= the proper subset of the set of all Paths
                which exist in the filesystem FS

  P in paths(FS) => exists(FS, P)

-----------  

  The root path, "/" is a directory, and must always exist in a filesystem
  
-----------

  
  isRoot(P) ::= P==[].
  
  forall FS in FileSystems : exists(FS,[]) && isDirectory(FS, []).
  forall FS in FileSystems : exists P in paths(FS) where isRoot(P)

-----------

  A path cannot refer to a file <and> a directory
    
-----------

  forall P in paths(FS) : isFile(FS, P) => !isDir(FS, P)
  forall P in paths(FS) : isFile(FS, P) => !isSymlink(FS, P)
  
  forall P in paths(FS) : isDir(FS, P) => !isFile(FS, P)
  forall P in paths(FS) : isDir(FS, P) => !isSymlink(FS, P)
  
  forall P in paths(FS) : isSymlink(FS, P) => !isFile(FS, P)
  forall P in paths(FS) : isSymlink(FS, P) => !isDir(FS, P)
  
-----------

  If a filesystem can contain other reference types, again, these MUST be mutually
  exclusive.
  
  Directories can be paths that have children, that is, there exist other paths
  in the filesystem whose path begins with a directory. This can be expressed
  by saying that every path's parent must be a directory.
  

   
  It can then be declared that a path has no parent
  -in which case it is the root directory,
  or it MUST have a parent that is a directory
   
-----------   
   
  forall P in paths(FS) : parent(P) == nil || isDir(FS, parent(P))
  
-----------
  
  Because the parent directories of all directories must themselves satisfy
  this criterion, it is implicit that only leaf nodes may be files 

  And, because every filesystem contains the root path, every filesystem
  must contain at least one directory.
  
-----------

  exists P in paths(FS) where isDir(FS, P)

-----------

  A directory may have children
  
-----------

  children(FS, P) ::= all C in paths(FS) where parent(C) == P

-----------

  There are no duplicate names in the child paths, because all paths are
  taken from the set of lists of path elements: there can be no duplicate entries
  in a set, hence no children with duplicate names.

  A path D is a descendent of a path P if it is the direct child of the
  path P or an ancestor is a direct child of path P
  
-----------

  isDescendent(P, D) ::= parent(D) == P where isDescendent(P, parent(D)) 
  
-----------

  The descendents of a directory P are all paths in the filesystem whose
  path begins with the path P -that is their parent is P or an ancestor is P

-----------

  descendents(FS, D) :: all P in paths(FS) where isDescendent(D,P) 
  
-----------

  Directories MUST not contain any data
  
-----------
  forall P in paths(FS): isDir(FS, P) => length(FS, P) == 0
  forall P in paths(FS): isDir(FS, P) => data(FS, P) == []
-----------

  Symbolic links MUST not contain any data
  
-----------
  forall P in paths(FS): isSymlink(FS, P) => length(FS, P) == 0
  forall P in paths(FS): isSymlink(FS, P) => data(FS, P) == []
-----------
  
  Files MAY contain data

-----------
  forall P in paths(FS): isFile(FS, P) => length(FS, P) >= 0
  forall P in paths(FS): isFile(FS, P) => (D = data(FS, P) && length(D)>=0) 
-----------

  Not covered: hard links in a filesystem. If a filesystem supports multiple
  references in paths(FS) to point to the same data, the outcome of operations
  are undefined.

  This model of a filesystem is sufficient to describe all the filesystem
  queries and manipulations -excluding metadata and permission operations.
  The Hadoop <<<FileSystem>>> and <<<FileContext>>> interfaces can be specified
  in terms of operations that query or change the state of a filesystem.
  

* org.apache.hadoop.fs.FileSystem
  
  All operations that take a Path to this interface MUST support relative paths.
  In such a case, they must be resolved relative to the working directory
  defined by <<<setWorkingDirectory()>>>.
      
** boolean exists(Path P)
    
-----------

  exists(FS, P)
  
-----------

** boolean isDirectory(Path P) 

-----------

  exists(FS, P) && isDir(FS, P)
  
-----------

** boolean isFile(Path P) 

-----------

  exists(FS, P) && isFile(FS, P)
  
-----------

** boolean isSymlink(Path P) 

-----------

  exists(FS, P) && isSymlink(FS, P)
  
-----------

** FileStatus getFileStatus(Path P)

  Preconditions

-----------
  
  exists(FS, P) || raise FileNotFoundException

-----------

  Postconditions

-----------

    (FS, FileStatus(length(F), isDirectory(F), [metadata], F)) )
  
-----------

** boolean mkdirs(Path F, FsPermission Permission )

  Preconditions

-----------
  
  isFile(F) => raise IOException
  isDir(F) => true
  mkdirs(parent(F))
  
-----------

  Postconditions

-----------
  
  FS' where (exists(FS', F) && isDir(FS', F))
  
-----------

  The probe for the existence and type of a path and directory creation MUST be
  atomic. The combined operation, including {{mkdirs(parent(F))}} MAY be atomic.


** <<<FileSystem.delete(Path P, boolean recursive)>>>

  Preconditions

-----------


  # a directory with children and recursive==false cannot be deleted
  # raise IOException
  isDir(FS, P) => (recursive || childen(FS, P) == {} )

  
-----------

  Postconditions

-----------

  #return false if file does not exist; FS state does not change
  !exists(FS, P) => (FS, false)
  
  # a path referring to a file is removed, return value: true
  isFile(FS, P) => (FS' where (!exists(FS', P)), true)


  
  # deleting an empty root returns true
  isDir(FS, P) && isRoot(P) && childen(FS, P) == {} 
    => (FS, true)) 

  # deleting an empty directory that is not will remove the path from the FS
  isDir(FS, P) && !isRoot(P) && childen(FS, P) == {} 
    => ((FS' where (!exists(FS', P) && !exists(descendents(FS', P))) , true) 


  # deleting a root path with children & recursive==true
  # removes the path and all descendents
  
  isDir(FS, P) && isRoot(P) && recursive  
    => (FS' where  !exists(descendents(FS', P)), true 
    
  # deleting a non-root path with children & recursive==true
  # removes the path and all descendents
  
  isDir(FS, P) && !isRoot(P) && recursive => 
   ( FS' where (!exists(FS', P) !exists(descendents(FS', P))): not parent(F,P))
  
-----------

  * Deleting a file is an atomic action.
  
  * Deleting an empty directory is atomic.
  
  * A recursive delete of a directory tree SHOULD be atomic. (or MUST?)


** <<<FileSystem.rename(Path S, Path D)>>>

  Rename includes the calculation of the destination path. 
  If the destination exists and is a directory, the final destination
  of the rename becomes the destination + the filename of the source path.
  
  
  D' = if (isDir(D) && D!=S) then (D :: filename(S)) else D.
  

  Preconditions

-----------

  #src cannot be root (special case of previous condition)
  !isRoot(S)

  # src must exist
  # raise: FileNotFoundException
  exists(FS, S)
  
  
  #dest cannot be a descendent of src
  !isDescendent(S, D')

  #dest must be root, or have a parent that exists
  isRoot(FS, D') || exists(FS, parent(D'))
  
  #parent must not be a file 
  !isFile(FS, parent(D'))
  
  # a destination can be a file iff source == dest
  # raise FileAlreadyExistsException
  !isFile(FS, D') || S==D'
  
  
-----------

  Postconditions

-----------

  #rename file to self is a no-op, returns true
  isFile(FS, S) && S==D' => (FS, true) 
    
  #renaming a dir to self is no op; return value is not specified
  # (posix => false, hdfs=> true)
  isDir(FS, S) && S==D' => (FS, ?)
  
  #renaming a file under dir adds file to dest dir, removes
  #old entry
  isFile(FS, S) && S!=D' =>
    FS' where (!exists(FS', S) && isFile(FS', D') && data(FS', D') == data(FS, S))
  
  # for a directory the entire tree under S exists under D, while 
  # S and its descendents do not exist
  isDir(FS, D) && S!=D' =>
    FS' where (
    (!exists(FS', S) 
      && isDir(FS', D')
      && forall C in descendents(FS, S) : !exists(FS', C)) 
      && forall C in descendents(FS, S) where isDir(C):
        exists C' in paths(FS) where isDir(C') 
        && childElements(D', C') == childElements(S, C)  
        && data(FS', C') == data(FS, C))
      )

-----------


*** Notes

    * rename() MUST be atomic

    * The behavior of <<<rename()>>> on an open file is unspecified.
    
    * The return code of renaming a directory to itself is unspecified. 


** `FileSystem.listStatus(Path P, PathFilter Filter)` 

  A <<<PathFilter)>>>) is a predicate function that returns true iff the path P
 meets the filter's requirements.

  Preconditions

-----------

  #path must exist
  exists(FS, P) || throw FileNotFoundException
  
-----------

  Postconditions
  
-----------

  isFile(FS, P) && Filter(P) => [FileStatus(FS, P)]
  
  isFile(FS, P) && !Filter(P) => []
  
  isDir(FS, P) => [all C in children(FS, P) where Filter(C)==true] 
  
-----------

  * After a file is created, all <<<`listStatus()>>>` operations on the file and parent
    directory MUST find the file.
  
  * After a file is deleted, all `<<<listStatus()>>>` operations on the file and parent
    directory MUST NOT find the file.
  
  * By the time the `<<<listStatus()>>>` operation returns to the caller, there
  is no guarantee that the information contained in the response is current.
  The details MAY be out of date -including the contents of any directory, the
  attributes of any files, and the existence of the path supplied. 


**  getFileBlockLocations(FileStatus F, int S, int L)

  Preconditions

-----------

  S >0  && L>=0  ||  throw InvalidArgumentException
  
-----------

  Postconditions
  
-----------

  F == null => null
  F !=null && F.getLen() <= S ==>  []
  
-----------

  If the filesystem is location aware, it must return the list
  of block locations where the data in the range S - S+L can be found.
  
  If the filesystem is not location aware, it SHOULD return

-----------

      [
        BlockLocation(["localhost:50010"] ,
                  ["localhost"],
                  ["/default/localhost"]
                   0, F.getLen())
       ] ;

-----------

  * A bug in Hadoop 1.0.3 means that a topology path of the same number
  of elements as the cluster topology MUST be provided, hence the 
  <<"/default/localhost">> path
  
  * HDFS throws <<<HadoopIllegalArgumentException>>> for an offset
  or length <0; as this extends IllegalArgumentException it is valid.
  
  * There is no implicit check for the FileStatus referring to a 
  directory. As FileStatus.getLen() for a directory is 0, it is
  implictly returning []


  REVIEW: MUST throw an `IOException` instance if `path.isDirectory()` is true
 
**  getFileBlockLocations(Path P, int S, int L)

  Preconditions

-----------

  P != null || throw NullPointerException
  exists(P) || throw FileNotFoundException
  
-----------

  Postconditions
  
-----------

  return getFileBlockLocations(getStatus(P), S, L)
  
  
-----------


**  getDefaultBlockSize(Path P), getDefaultBlockSize();

  Preconditions

-----------
  
-----------

  Postconditions
  
-----------

  return integer  >= 0 
  
-----------

  Although there is no required value for this, as it
  is used to partition work during job submission, a block size
  that is too small will result in either too many jobs being submitted
  for efficient work, or the <<<JobSubmissionClient>>> running out of memory.
  Any FileSystem that does not break files into block sizes SHOULD
  return a number for this that results in efficient processing. 
  (it MAY make this user-configurable)


** FSDataOutputStream create(Path f, ...)

-----------

FSDataOutputStream create(Path P,
      FsPermission permission,
      boolean overwrite,
      int bufferSize,
      short replication,
      long blockSize,
      Progressable progress) throws IOException;

-----------

  Preconditions

-----------

    # MUST raise FileAlreadyExistsException, FileNotFoundException
    ! exists(P) || (overwrite && isFile(P))
    isDir(parent(P)) || mkdirs(parent(P))

-----------

  Postconditions
  
-----------

    FS' where (isFile(P))) 
  
-----------

  Return: <<<FSOutputStream>>>, 

  * Filesystems may reject the request for other
  reasons -such as the FS being read-only  (HDFS), 
  the block size being below the minimum permitted (HDFS),
  the replication count being out of range (HDFS),
  quotas on namespace or filesystem being exceeded, reserved
  names, ...etc. All rejections SHOULD be IOException or a subclass thereof
  and MAY be a RuntimeException or subclass. (HDFS: InvalidPathException)
  
  * S3N, Swift and other blobstores do not currently change the FS state
  until the output stream is closed(). This MAY be a bug, as it allows >1
  client to create a file with overwrite=false
  
** FSDataOutputStream append(Path f, int bufferSize, Progressable progress

  Implementations MAY throw {{UnsupportedOperationException}}

  Preconditions

-----------

    exists(P) || throw  FileNotFoundException
    isFile(P))|| throw  IOException or subclass, including FileNotFoundException

-----------

  Postconditions
  
-----------
  
-----------

  Return: <<<FSOutputStream>>>, 

** concat(Path f, int bufferSize, Progressable progress

  Implementations MAY throw {{UnsupportedOperationException}}

  Preconditions

-----------

    exists(P) || throw  FileNotFoundException
    isFile(P))|| throw  IOException or subclass, including FileNotFoundException

-----------

  Postconditions
  
-----------
  
-----------

  Return: <<<FSOutputStream>>>, 

~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* InputStream, Seekable and PositionedReadable

** Invariants


  [[1]] After all operations on <<<Seekable>>>, <<<InputStream>>> and
   <<<PositionedReadable>>> of a file <F>, 
   <<<length(F)>>> MUST be unchanged, and the contents of the file <F> must be unchanged.
   Metadata about the file (e.g. the access time) MAY change.

** Concurrency

  [[1]] All operations on an <<<InputStream>>> <S> are assumed to be thread-unsafe.

  [[1]] If the file which is being accessed is changed during a series
  of operations, the outcome is not defined.

** InputStream

  Implementations of <<<InputStream>>> MUST follow the Java specification of
  <<<InputStream>>>. For an <<<InputStream>>> <S> created from
  <<<FileSystem.open(F: Path): InputStream>>>, the following conditions MUST hold

    [[1]] After <<<S.close())>>> all <<<read>>> operations are SHOULD fail with
    an exception. It MAY NOT, in which case the outcome of all futher operations
    are undefined.
  
    [[1]]<<<S.read()>>> where less than <<<length(F)>>> bytes have been served.
    MUST return the next byte of data
  
    [[1]] <<<S.read()>>> where more than <<<length(F)>>> bytes have been served.
    MUST return -1.
  
    [[1]] <<<S.read(Buffer, Offset, Len)>>>
    where <<<Offset < 0 ||  Offset>>> >
     <<<Buffer.length || (Len >>> >
      <<<(Buffer.length - Offset)) || Len <0 >>>
    MUST throw
    <<<InvalidArgumentException>>> or another <<RuntimeException>>
    including -but not limited to- <<<ArrayIndexOutOfBoundsException>>>
  
    [[1]] <<<S.read(null, int Offset, int Len )>>> MUST throw
    <<<NullPointerException>>>.
  
    [[1]] <<<S.read(Buffer, Offset, 0 )>>> MUST return 0 if
    the preceeding criteria have been met.
  
    [[1]] <<<S.read(Buffer, Offset, Len)>>>
    MUST fill <<<Buffer>>> with <<<min(Len, length(F)-Len)>>> entries from
    the input stream defined by <<<S.read()>>>
    and return the number of bytes written.
  
    [[1]] <<<S.read(Buffer, Offset, Len)>>>
    where S has already streamed <<<length(F)>>> bytes MUST return <<<-1>>>
    and MUST make no changes to the contents of <<<Buffer>>>.

  Notes:

    [[1]] the <<<InputStream>>> definition does not place any limit
    on how long <<read())>> may take to complete.

** Seekable

  The interface <<<Seekable>>> MAY be implemented by classes that extend
  <<<InputStream>>> and SHOULD NOT be implemented that classes that do not.


+---------------------------------------------

public interface Seekable {
  void seek(long pos) throws IOException;
  long getPos() throws IOException;
  boolean seekToNewSource(long targetPos) throws IOException;
}

+---------------------------------------------

  For an <<<InputStream>>> <S> created from
  <<<FileSystem.open(F: Path): InputStream>>>, where <<<S instanceof Seekable>>>
  is true and <<<S.seek()>>> does not throw an <<<UnsupportedOperation>>> exemption,
  the following conditions MUST hold:-

    [[1]] For a newly opened input stream <S>, <<<S.getPos()>>> MUST
    equal 0.
  
    [[1]] For all values of <P> in <Long> where <<<S.seek(P)>>> does 
    throw an exception, the outcomes of <<<S.getPos()>>> and <<<S.read()>>>
    operations are undefined.
  
    [[1]] After <<<S.close()>>>, <<<S.seek(P)>>> MUST fail with an <<<IOException>>>
  
    [[1]] For all values of <P1>, <P2> in <Long>,  <<<S.seek(P1)>>> followed
    by <<<S.seek(P2)>>> is the equivalent of <<<S.seek(P2)>>>.
    <this only holds if S.seek(P1) does not raise an exception -can we explicitly
    declare that this elision can ignore that possibility?>
  
    [[1]] On <<<S.seek(P)>>> with <P < 0> an exception MUST be thrown.
    It SHOULD be an <<<IOException>>>. It MAY be an <<<IllegalArgumentException >>>
    or other <<<RuntimeException>>>.
  
    [[1]] On <<<S.seek(P)>>> with <P>><length(F)>, an <<<IOException>>> MAY be thrown.
  
    [[1]]  For all values of <P> in <Long> if <<<S.seek(P)>>> does not raise
     an Exception, it is considered a successful seek.
  
    [[1]] For all successful <<<S.seek(P)>>> operations <<<S.getPos()>>> MUST equal <P>
  
    [[1]] This implies that a successful <<<S.seek(S.getPos())>>> does not
    change the cursor position with in the file. This is considered a no-op
    that MAY therefore be bypassed completely.
  
    [[1]] For all S, P where <P = S.getPos()> and <P < length(F)>,
    after <<<S.read())>>> , <<<S.getPos()>>> MUST equal <<<P+1>>>.
  
    [[1]] After <<<S.seek(P)>>> with <P> >=<length(F)>, If an exception is not thrown 
    then <<<S.read()>>> MUST equal -1. The value of <<<S.getPos()>>> MUST be unchanged.
  
    [[1]] If an exception is not thrown after <<<S.seek(P)>>> with <P> >=<length(F)>,
    then <<<S.read(Buffer, Offset, Len)>>> MUST equal -1.
    The value of <<<S.getPos()>>> MUST be unchanged.
  
    [[1]] On <<<S.seek(P)>>> with < P >length(F) >,
    if an <<<IOException>>> is not
    thrown, then <<<S.read()>>> and
    <<<S.read(byte[] buffer, int offset, int len )>>>
    operation MUST return -1.
  
    [[1]] On <<<S.seek(P)>>> with <P> ><length(F)>,
    if an <<<IOException>>> is not
    thrown, <<<S.read(Buffer, Offset, Len )>>>
    operation MUST return -1, and the contents of <Buffer> unchanged.
  
    [[1]] After a <<<S.seek(P)>>> with <0<=P<length(file)>,
     <<<read(Buffer,0,1)>>> MUST set <<<Buffer[0]>>>
      to the byte at position <<<P>>> in the file,
     and <<<S.getPos()>>> MUST equal <<<P+1>>>
     
    [[1]] <<<S.seekToNewSource(Pos)>>> SHOULD return false if the
    filesystem does not implement multiple-data sources for files.
    
    [[1]] After <<<S.seekToNewSource(Pos)>>> the value of <<<S.getPos()>>>
    MUST be unchanged
    

  Irrespective of whether or a seekToNewSource() operation succeeds or fails,
the stream's getPos() value MUST be the value which it was before the seek
operation was invoked. Specifically, it is not a seek() operation, it is a
request to bind to a new location of data in expectation of a read or seek
operation fetching the new data.

* PositionedReadable

-------------

public interface PositionedReadable {
  public int read(long position, byte[] buffer, int offset, int length)
    throws IOException;
  public void readFully(long position, byte[] buffer, int offset, int length)
    throws IOException;
  public void readFully(long position, byte[] buffer) throws IOException;
}

-------------

    [[1]] The result <R> of <<<S.read(Pos, Buffer, Offset, Len )>>> MUST be identical
    to the sequence

-----------

  P0 = S.getPos()
  try {
    S.seek(Pos)
    R = S.read( Buffer, Offset, Len )
  } finally {
    S.seek(P0)
  }
  
-----------
  
    [[1]] Degenerate case: The outcome of <<<S.read(S.getPos()), Buffer, Offset, Len )>>>
    MUST be identical to that of the operation <<<S.read( Buffer, Offset, Len )>>>.
    
    [[1]] The outcome of <<<S.readFully(Pos, Buffer, Offset, Len )>>>
    where <<<Pos + Len >> is less than the length of the file MUST be the same
    as <<<S.read(Pos, Buffer, Offset, Len )>>>.
  
    [[1]] <<<S.readFully(Pos, Buffer, Offset, Len )>>>
    where <<<Pos + Len >> is greater than the length of the file MUST raise an
    <<<EOFException>>>.
    
    [[1]] <<<S.readFully(Pos, Buffer)>>> MUST be equivalent
    to <<<S.readFully(Pos, Buffer, 0, Buffer.length )>>>
    
  
~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


* Operations and failures

    * All operations MUST eventually complete, successfully or unsuccessfully,
      throw an <<<IOException>>> or subclass thereof.

    * The time to complete an operation is undefined and may depend on
    the implementation and on the state of the system.

    * Operations MAY throw a <<<RuntimeException>>> or subclass thereof.

    * Operations SHOULD raise all network, remote and high-level problems as
    an <<<IOException>>> or subclass thereof, and SHOULD NOT raise a
    <<<RuntimeException>>> for such problems.

    * Operations SHOULD report failures by way of raised exceptions, rather
    than specific return codes of an operation.

    * In the text, when an exception class is named, such as <<<IOException>>>,
    the raised exception MAY be an instance of or subclass of the named exception.
    It MUST NOT be a superclass

    * If an operation is not implemented in a class, the implementation must
    throw an <<<UnsupportedOperationException>>>
    

